---
title: 17-扩散理论的数学研究
published: 2024-03-01
description: '本文主要研究了扩散算法的数学原理，包含前向过程和后向过程。'
tags: [Computer Vision, Diffusion]
category: '知识总结'
draft: false 
---
# 一、之前的研读总结

        笔者之前进行的研读大多是针对“差分隐私”的数学推导。这段时间内研读的方向主要包括以下几个方面：

- 利用 **差分隐私+同态加密** 的方式，进行各种场景的实际应用

- **联邦学习**，利用改进的学习过程，保护数据传输阶段的隐私

- **深度学习DP**，在保证隐私保护程度的前提下，提高精度

- 通过其他概率论手段，**发布一个新的数据集**，这个数据集既能够符合原数据集的分布规律，又能保证差分隐私规范。

    笔者感觉，第四个方面具有较大的挑战性，但同时也是属于比较有创新点的。它属于“发布数据集”的范畴，需要同时统筹以下方面：

    - “**分布规律一致性**”：发布数据集和原数据集的分布规律保持一致

    - “**差分隐私规范性**”：发布的数据集需要符合差分隐私的规范

    - “**数据自身一致性**”：在往生成数据集中添加噪声时会打破一致性（例如概率和可能为负或者超过1，大量的低噪会让一些低概率的事情提高概率性，进而打扰分布一致）

    - “**精度要尽可能高**”：基于这些数据集获得的训练成果，精度要尽可能高

        不难发现，以上几个条件彼此之间是互相矛盾的：**为了符合差分隐私的规范，不得不往数据中添加大量的噪声。而大量噪声又会打破数据集的一致性以及不可避免地降低训练精度**。理论上来讲，这应该是一个不可避免的矛盾。

        但是，在最近比较火热的 diffusion 领域，笔者感觉这种矛盾并不是尖锐对立的。为了保护数据隐私而添加噪声的行为，在 diffusion 上或许反而能够提高生成图像的质量。

        在笔者尝试利用 stable-diffusion 绘制图像时，我发现**生成的图像在局部有着很惊艳的质量，但如果聚焦于图像全局，则经常会出现“畸形”**，例如“生成的人物总是多一只手或者脚”、“生成人物的手指数量不对”、“人物的明暗关系、透视不正确”…….此外，“**版权**”的问题也是一个值得商榷的问题。

        造成这种问题的原因应该是由于**生成算法陷入了“局部最优解”**。一般情况下，对于局部最优解，**“添加噪声”的方式本身就有助于“跳出”局部解**。此外，我认为，在“图像生成”这个领域，**“整体的协调性”比“局部的精细度”更加重要**。一个局部粗糙，但是符合人体结构的生成算法，显然会比局部精细但是人体结构崩坏的算法，更加符合现实审美。

        基于这一点，如果我们使得“噪声”的加入在牺牲“局部精细度”的时候，高度保持了“整体的协调性”，那么或许反而能够提高 Diffusion 算法的适用程度。具体来讲，Diffusion 算法中应该有一个类似”损失函数“的东西来控制精度（这个算法的实现原理将是接下来研读的重点）。如果我们能够**将这个”精度“分解成”局部精细度“和”全局一致性“**，那么加入噪声的时候，侧重点偏向于”局部精细度“，而保持”全局一致性“的精度，那么我们便能够在一定程度上缓解 Diffusion 算法生成”畸形“图像的问题了。

        此外，现有的很多”AI绘图“平台，它们所基于的训练素材大多是不存在合法版权的（例如禁止商用等等），因而生成的图像如果存在过拟合的问题，那么便可以称之为”侵权“。而”差分隐私“算法的引入，或许也能够解决”过拟合“的问题。

        基于以上两种的思想，我决定去深入研究一下 DDPM 相关的论文，希望能够从中找到相关的线索。此外，Diffusion 算法本身也是属于”生成数据集“的一种，也可以和 DP 结合，在”基于DP的合成数据集“相关的领域对我应该也有相关的启发。

        总结一下，本次研读的目标：

- 了解 Diffusion 算法的**实现原理**；

- **给出一个标准，用于划分 Diffusion 中对于”精度“的损失函数**，将其划分为”全局一致性“和”局部精细度“；

- 探究 Diffusion 算法和差分隐私结合的可能性，实现”**保障全局一致性**“的前提下，通过“**牺牲局部精细度**“来**保证隐私保护**（类似于DP中的”局部敏感“和”全局敏感“的调和）；

- 在上一点的基础上，**将”精度的划分“思想模型完善**，尝试能否在一些简单的方法上进行运用。

# 二、Diffusion 研读

        DDPM模型包括了两个过程：前向过程和反向过程。它的每一个过程都是一个参数化的马尔可夫链。在前向过程中，对原始图片 $X_0$ 不断添加高斯噪声，最后生成了一个完全随机的噪声 $X_t$ 。而逆向过程则反过来，通过一个随机的高斯噪声来生成图片。

## 2.1 前向过程

        在前向过程中，我们把第 t 次添加噪声后的结果图片记为 $X_t$，则我们可以给出如下所示的马尔可夫传递过程：

$$
X_t=\sqrt{\alpha_t}\;X_{t-1}+\sqrt{1-\alpha_t}\;\epsilon_{t-1}

$$

        其中，$ε_{t-1} \sim N(0,1)$ ，是一个高斯噪声。我们继续迭代一次 $X_{t-1}$，可以得到：

$$
X_t=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\;X_{t-2}+\sqrt{1-\alpha_{t-1}}\;\epsilon_{t-2})+\sqrt{1-\alpha_t}\;\epsilon_{t-1}\\X_t=\sqrt{\alpha_t\;\alpha_{t-1}}\;X_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}\;\epsilon_{t-2}+\sqrt{1-\alpha_t}\;\epsilon_{t-1}

$$

        到这一步为止，我们发现式子末尾的噪声部分非常复杂，在此，我们引入高斯分布的两条性质：

$$
定理一、

如果 X \sim N(\mu, \sigma^2)，且 a 和 b 都是实数，那么 aX+b \sim N(a\mu+b, (a\sigma)^2)\\定理二、如果 X \sim N(\mu_X, \sigma^2_X)，Y \sim N(\mu_Y, \sigma^2_Y)，且 X 和 Y 是统计独立的正态随机变量，那么它们的和也满足正态分布 U = X+Y  \sim N(\mu_X+\mu_Y, \sigma^2_X+\sigma^2_Y)



$$

        由于 $ε_{t-1}, ε_{t-2} \sim N( 0,1 )$ ，因此根据 定理一 有：

$$
\sqrt{\alpha_t(1-\alpha_{t-1})}\;\epsilon_{t-2} \sim N(0,\alpha_t(1-\alpha_{t-1}))\\\sqrt{1-\alpha_t}\;\epsilon_{t-1} \sim N(0,1-\alpha_t)

$$

        再根据 定理二，它们相加的结果也服从高斯分布：

$$
\sqrt{\alpha_t(1-\alpha_{t-1})}\;\epsilon_{t-2}+\sqrt{1-\alpha_t}\;\epsilon_{t-1} \sim N(0,1-\alpha_t\alpha_{t-1})

$$

        这样，我们就可以把 X_t 的迭代写成统一的规范：

$$
X_t=\sqrt{\alpha_t\alpha_{t-1}}\;X_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\;\epsilon'

$$

        这样不断迭代下去，我们就可以直接写出 $X_t$ 和 $X_0$ 之间的关系：

$$
令 \;\overline{\alpha_t}=\alpha_t\alpha_{t-1}...\alpha_1，那么有：X_t=\sqrt{\overline{\alpha_t}}\;X_0+\sqrt{1-\overline{\alpha_t}}\;\epsilon

$$

        根据上述这道公式，给定图片$X_0$，确定每一步的参数，我们就可以直接计算出最终得到的随机噪声 $X_t$，这就大大加快了采样的过程。

## 2.2 反向过程

        这里困扰了我很长时间。既然正向过程我们已经得到了，那为什么逆向过程不能直接反转一下，把 $X_{t-1}$ 移到左边，$X_t$ 移动到右边，用和上面相似的推导过程快速得出前向的图像 $X_i$ 呢？

        我的理解是这样的。首先，我们假设 $X_0$  是一个图片的状态，而 $X_t$ 是一个噪声的状态。那么从  $X_0$  到 $X_t$ ，在这个过程里有效信息被丢失了，整体分布的“有序性”降低，因而呈现出了“熵增”的特性。而“熵增”本身是一个不可逆的过程。这正和它的名称“diffusion”一样，物理学中的“扩散”现象，本质也是熵增的过程，如果没有外力作用，那么这个过程就是不可逆的。

        从 $X_0$ 到 $X_t$，我们根据马尔可夫过程，构建了一个高斯分布 $q(X_t | X_{t-1})$。在逆向过程中，我们需要的是一个逆向分布 $q(X_{t-1} | X_t)$，这样可以利用这个逆向分布来从噪声生成图像。但是这个逆向分布究竟是不是高斯分布，这个我们是不知道的，需要去证明。如果直接反转，认为 $q(X_{t-1} | X_t)$ 也是一个高斯分布，用上述的式子逆推的话，可能就会导致信息的损失和精度的下降。

        因此，逆向过程应该理解为，我们在有数据 $X_t$ 和 $X_{t-1}$ 的前提下，需要构建一个神经网络，来模拟从 $X_t$ 到 $X_{t-1}$ 的过程。

        我们使用参数化的网络来建模这个去噪过程。下面的这个式子是为了近似目标分布而构建的：

$$
p_{\theta}(X_{0:T})\;=\;p(X_T)\Pi_{t=1}^{T}\;p(X_{t-1}|X_t)\\q_{\theta}(X_{t-1}|X_{t})=N(X_{t-1};\mu_{\theta}(X_t,t),\Sigma_\theta(X_t,t))

$$

        需要注意的是，当$X_0$已知的时候，反向的条件概率是可以求出来的，我们利用贝叶斯公式做如下变换：

$$
q(X_{t-1}\;|\;X_t,X_0)=\frac{q(X_{t-1},X_t,X_0)}{q(X_t,X_0)}

$$

        不难发现，原本是从  $t \rightarrow t-1$ 的逆向过程，在这里的分母中，却演化成了从 $0 \rightarrow t$ 的正向过程。同理，如果我们能够把分子也变成正向过程，那么就可以推导出在 $X_0$ 和 $X_t$ 已知条件下，$X_{t-1}$ 的概率了。

        继续利用贝叶斯公式进行如下变换：

$$
q(X_{t-1},X_t,X_0)=q(X_t,X_{t-1},X_0)=q(X_t\;|\;X_{t-1},X_0)*q(X_{t-1},X_0)

$$

        对分子和分母同时进行进一步变换：

$$
q(X_t,X_0）=q(X_t|X_0)*q(X_0)\\q(X_{t-1},X_{0})=q(X_{t-1}|X_0)*q(X_0)

$$

        削去同项 $q(X_0)$，得到：

$$
q(X_{t-1}|X_t,X_0)=q(X_t|X_{t-1},X_0)\frac{q(X_{t-1}|X_0)}{q(X_{t}|X_0)}

$$

        这样我们就把一个逆向过程变成了正向过程的四则运算。

        注意到在正向过程中，有：

$$
q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI),\beta_t=1-\alpha_t

$$

---

        **PS：采样过程遵循封闭公式。这个技巧也被称作“重参数”技巧：**

$$
IF \;\;z \sim N(\mu,\sigma^2),\;then \;z=\mu+\sigma\epsilon\;\;where \;\;\epsilon\sim N(0,1)

$$

        这样对于上述的 $q(x_t|x_{t-1})$，便得到了一开始的马尔可夫传递过程：

$$
X_t=\sqrt{\alpha_t}\;X_{t-1}+\sqrt{1-\alpha_t}\;\epsilon_{t-1}

$$

---

        这样，上面正向过程运算的各个部分便可以细化为具体的高斯分布：

$$
q(X_t|X_{t-1},X_{0})=\frac{1}{\sqrt{2\pi}\sqrt{1-\alpha_t}}exp(-\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{2\beta_t})

$$

$$
q(X_{t-1}|X_0)=\frac{1}{\sqrt{2\pi}\sqrt{1-\overline{\alpha}_{t-1}}}exp(\frac{(x_{t-1}-\sqrt{\overline{\alpha}_{t-1}}x_0)^2}{2(1-\overline{\alpha}_{t-1})})

$$

        因为 $\alpha_i$为常数，因此自然常数项前的这一项可以统统约化为一个常量C，从而有：

$$
q(X_{t-1}|X_t,X_0) \propto  exp(-\frac{1}{2}(\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{\beta_t})+\frac{(x_{t-1}-\sqrt{\overline{\alpha}_{t-1}}x_0)^2}{1-\overline{\alpha}_{t-1}}-\frac{(x_{t}-\sqrt{\overline{\alpha}_{t}}x_0)^2}{1-\overline{\alpha}_{t}})

$$

        将上式展开，合并同类项，有：

$$
q(X_{t-1}|X_t,X_0) \propto exp\{-\frac{1}{2}[(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha}_{t-1}})x_{t-1}^2-(\frac{2\sqrt{\alpha_t}}{\beta_t}x_t+\frac{2\sqrt{\overline{\alpha}_{t-1}}}{1-\overline{\alpha}_{t-1}}x_0)x_{t-1}+C(x_t,x_0)]\}

$$

        而 $q(X_{t-1}|X_t,X_0)$ 本身遵循高斯分布，也即：

$$
q(X_{t-1}|X_t,X_0) = N(x_{t-1};\hat{\mu}(x_t,x_0),\hat{\beta_t}I)

$$

        根据高斯分布公式的特点，我们便可以一一写出上式中的对应部分：

$$
exp(-\frac{(x-\mu)^2}{2\sigma^2})=exp(-\frac{1}{2}(\frac{1}{\sigma^2}x^2-\frac{2\mu}{\sigma^2}x+\frac{\mu^2}{\sigma^2}))

$$

        方差部分：

$$
\frac{1}{\sigma^2}=\frac{1}{\hat{\beta_t}}=\frac{\alpha_t}{\beta_t}+\frac{1}{1-\overline{\alpha}_{t-1}}, \; \hat{\beta_t}=\frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}*\beta_t

$$

        期望部分：

$$
\frac{2\mu}{\sigma^2}=\frac{2\hat{\mu_t}(x_t,x_0)}{\hat{\beta_t}}=(\frac{2\sqrt{\alpha_t}}{\beta_t}x_t+\frac{2\sqrt{\overline{\alpha}_{t-1}}}{1-\overline{\alpha}_{t-1}}x_0)

$$

$$
\hat{\mu_t}(x_t,x_0)=\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t+\frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t}x_0

$$

        在 $X_0$ 已知的前提（正向过程倒置）下：

$$
X_t=\sqrt{\overline{\alpha_t}}\;X_0+\sqrt{1-\overline{\alpha_t}}\;\epsilon\\X_0=\frac{1}{\sqrt{\overline{\alpha}_t}}(x_t-\sqrt{1-\overline{\alpha}_t}\overline{z}_t)

$$

        将结果代入上式，可以得到：

$$
\hat{\mu}_t=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}_t}}\overline{z}_t)

$$

        也即：

$$
\hat{\mu}(x_t,t)=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}_t}}z_\theta(x_t,t))

$$

        在此，我们得到了数学期望、方差，带入到一开始构建的网络中，便可以进行训练了：

$$
q_{\theta}(X_{t-1}|X_{t})=N(X_{t-1};\mu_{\theta}(X_t,t),\Sigma_\theta(X_t,t))

$$

        每一步的推断可以总结为：

- 每个时间步通过当前的 $x_t$ 和 $t$ 来预测高斯噪声，然后根据上式得到数学期望 $\hat{\mu}(x_t,t)$

- 得到方差 $Σθ(x_t,t)$ ，DDPM中使用untrained $Σθ(x_t,t)=\hat{β_t}$ ，且认为 $\hat{\beta}_t = \beta_t$ 和 $\hat{\beta}_t = \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}\beta_t$ 结果近似

- 计算得到 $q(x_{t−1}|x_t)$ ，利用重参数技巧得到 $x_{t−1}$
